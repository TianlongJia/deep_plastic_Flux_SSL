{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W65GPvi1XDsl"
   },
   "source": [
    "# Oject detection evaluation with SAHI\n",
    "\n",
    "Test a Detector on A Customized Dataset using Slicing Aided Hyper Inference (SAHI). Here, I use the Faster RCNN\n",
    "\n",
    "https://github.com/obss/sahi\n",
    "\n",
    "https://medium.com/voxel51/how-to-detect-small-objects-cfa569b4d5bd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCl2HeNi0wfY"
   },
   "source": [
    "- Import required modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "\n",
    "from sahi.utils.detectron2 import Detectron2TestConstants\n",
    "\n",
    "# import required functions, classes\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.predict import get_sliced_prediction, predict, get_prediction\n",
    "from sahi.utils.file import download_from_url\n",
    "from sahi.utils.cv import read_image\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_path = \"/scratch/tjian/PythonProject/deep_plastic_Flux_SSL/checkpoint/train_weights/RN50_500K/Exp1/RN50_500K_200e/SSL_100per_1/model_best_83.6285.pth\"\n",
    "model_config_path = \"/scratch/tjian/PythonProject/deep_plastic_Flux_SSL/checkpoint/train_weights/RN50_500K/Exp1/RN50_500K_200e/SSL_100per_1/config.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register my custom train and test dataset\n",
    "\n",
    "dataset should be in COCO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register a test dataset\n",
    "register_coco_instances(\"Flux_train_100per\", {}, \"/scratch/tjian/Data/Flux/labeled_images_new/annotations/Train_100per.json\", \"/scratch/tjian/Data/Flux/labeled_images_new/Train_100per/\")\n",
    "register_coco_instances(\"Flux_val_100per\", {}, \"/scratch/tjian/Data/Flux/labeled_images_new/annotations/Val_100per.json\", \"/scratch/tjian/Data/Flux/labeled_images_new/Val_100per/\")\n",
    "Train_Dataset_name=\"Flux_train_100per\"\n",
    "Test_Dataset_name=\"Flux_val_100per\"\n",
    "Class_name=[\"litter\"]\n",
    "\n",
    "MetadataCatalog.get(Train_Dataset_name).set(thing_classes=Class_name)\n",
    "MetadataCatalog.get(Test_Dataset_name).set(thing_classes=Class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a detection model\n",
    "\n",
    "Instantiate a detection model by defining model weight path, confing path and other parameters.\n",
    "\n",
    "This model will be used to inference in slicing images or original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"image_size\": Input image size for each inference (image is scaled by preserving aspect ratio).\n",
    "# there is no default \"image_size\". \n",
    "# So if not set \"image_size\", the image will be resized by the setting in the model config file\n",
    "\n",
    "# \"device\": (1) \"cpu\", or (2) 'cuda:0'\n",
    "\n",
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='detectron2',\n",
    "    model_path=model_checkpoint_path,\n",
    "    config_path=model_config_path,\n",
    "    confidence_threshold=0.9,\n",
    "    # image_size=1333,\n",
    "    device=\"cuda:0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliced Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Predict an image\n",
    "\n",
    "Sliced Inference with a Detectron2 Model\n",
    "\n",
    "- To perform sliced prediction we need to specify slice parameters. In this example we will perform prediction over slices of slice_width*slice_height (e.g.,256x256) with an overlap ratio of 0.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BM6uCgAS6Ze4",
    "outputId": "a703b758-0141-4ac7-f7b2-0804f7b10070"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing prediction on 15 number of slices.\n"
     ]
    }
   ],
   "source": [
    "# define image path\n",
    "image_path=r\"/scratch/tjian/Data/Flux/TUD_Vietnam/0609/images_with_litter/BL_2023_09_06_Sweep_1_5_DCSS0262.jpg\"\n",
    "\n",
    "export_dir=r\"/scratch/tjian/Data/Flux/TUD_Vietnam/0609/Predict/R50_300K_100e/SAHI/test/\"\n",
    "output_file_name= \"BL_2023_09_06_Sweep_1_5_DCSS0262\"   # do not end with \".jpg\"\n",
    "\n",
    "result = get_sliced_prediction(\n",
    "    image_path,\n",
    "    detection_model,\n",
    "    slice_height = 640,\n",
    "    slice_width = 640,\n",
    "    overlap_height_ratio = 0.2,\n",
    "    overlap_width_ratio = 0.2,\n",
    ")\n",
    "\n",
    "result.export_visuals(export_dir, \n",
    "                      text_size=3,\n",
    "                      file_name=output_file_name)\n",
    "\n",
    "# output No. detections\n",
    "num_sliced_dets = len(result.to_fiftyone_detections())\n",
    "print(f\"Detections predicted with slicing: {num_sliced_dets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Predict images in a folder\n",
    "\n",
    "Output:\n",
    "\n",
    "(1) predicted images (the code will save the predicted images in a subfolder named *\"pred_images\"*); \n",
    "\n",
    "(2) A excel file named *\"detection_results.xlsx\"*, stores filename and No.detections\n",
    "\n",
    "(3) Print the total number of detections in the console\n",
    "\n",
    "\"postprocess_type\": Options are 'NMM', 'GREEDYNMM', 'LSNMS' or 'NMS'. Default is 'GREEDYNMM'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image_dir = r\"/scratch/tjian/Data/Flux/TUD_Vietnam/1209/images_no_litter/\"\n",
    "\n",
    "# do not generate the output_dir, the code will do it automatically\n",
    "output_dir = r\"/scratch/tjian/Data/Flux/TUD_Vietnam/1209/Pred/SSL_20per_7/SA_1920_Conf_0.9/No_litter\"\n",
    "\n",
    "# define slicing conf.\n",
    "slice_height = 1920\n",
    "slice_width = 1920\n",
    "overlap_height_ratio = 0.2\n",
    "overlap_width_ratio = 0.2\n",
    "\n",
    "# Perform sliced inference on given folder\n",
    "# it will also output the (total) number of detections in each/all images\n",
    "result = predict(\n",
    "    detection_model = detection_model,\n",
    "    source = source_image_dir,\n",
    "    slice_height = slice_height,\n",
    "    slice_width = slice_width,\n",
    "    overlap_height_ratio = overlap_height_ratio,\n",
    "    overlap_width_ratio = overlap_width_ratio,\n",
    "    project = output_dir,\n",
    "    postprocess_type = 'NMS', # dDefault is 'GREEDYNMM'.\n",
    "    postprocess_match_metric = 'IOU',\n",
    "    postprocess_match_threshold = 0.5, # NMS_IoU_threshold\n",
    "    visual_text_size = 3 # float\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluate models on a labeled dataset\n",
    "\n",
    "In the output path, the code generates:ã€€\n",
    "\n",
    "(1) predicted images (the code will save the predicted images in a subfolder named *\"pred_images\"*); \n",
    "\n",
    "(2) a subfolder named *\"visuals_with_gt\"* including images with ground-truth and predictions;\n",
    "\n",
    "(3) a json file \"results.json\" including prediction results;\n",
    "\n",
    "(4) A excel file named *\"detection_results.xlsx\"*, stores filename and No.detections\n",
    "\n",
    "(5) Print the total number of detections in the console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Processing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_dir = r\"/scratch/tjian/Data/Flux/TUD_Vietnam/1209/Sweep_4_5/images/\"\n",
    "\n",
    "# test dataset (with annotations)\n",
    "test_annotation_path = r\"/scratch/tjian/Data/Flux/TUD_Vietnam/1209/Sweep_4_5/Sweep_4_5.json\"\n",
    "\n",
    "# do not generate the output_dir, the code will do it automatically\n",
    "output_dir = r\"/scratch/tjian/Data/Flux/TUD_Vietnam/1209/Sweep_4_5/Pred/\"\n",
    "\n",
    "# define slicing conf.\n",
    "slice_height = 1920\n",
    "slice_width = 1920\n",
    "overlap_height_ratio = 0.2\n",
    "overlap_width_ratio = 0.2\n",
    "\n",
    "# Perform sliced inference on given folder:\n",
    "# it will also output the (total) number of detections in each/all images\n",
    "predict(\n",
    "    detection_model = detection_model,\n",
    "    source = test_image_dir,\n",
    "    slice_height = slice_height,\n",
    "    slice_width = slice_width,\n",
    "    overlap_height_ratio = overlap_height_ratio,\n",
    "    overlap_width_ratio = overlap_width_ratio,\n",
    "    project = output_dir,\n",
    "    dataset_json_path = test_annotation_path,\n",
    "    postprocess_type = 'NMS', # dDefault is 'GREEDYNMM'.\n",
    "    postprocess_match_metric = 'IOU',\n",
    "    postprocess_match_threshold = 0.5, # NMS_IoU_threshold\n",
    "    visual_text_size = 3 # float\n",
    "    # visual_bbox_thickness = None  # init\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Modify the output json file\n",
    "\n",
    "Note: the \"results.json\" file output in the above code (1) has an issue of category. \n",
    "\n",
    "By default, Detectron2 or SAHI starts the category_id from \"0\". But the category_id starts from \"1\" in my input annotation file. \n",
    "\n",
    "Thus, it is **necessary** that all category IDs are incremented by 1 in the output json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "input_json = r'/scratch/tjian/Data/Flux/TUD_Vietnam/1209/Sweep_4_5/Pred/results.json'\n",
    "output_json = r'/scratch/tjian/Data/Flux/TUD_Vietnam/1209/Sweep_4_5/Pred/results_new.json'\n",
    "\n",
    "# Read the JSON file\n",
    "with open(input_json, 'r') as file:\n",
    "    coco_data = json.load(file)\n",
    "\n",
    "# Check if the JSON structure is a list or a dictionary\n",
    "if isinstance(coco_data, list):\n",
    "    # Assume the list contains annotation dictionaries directly\n",
    "    for annotation in coco_data:\n",
    "        annotation['category_id'] += 1\n",
    "else:\n",
    "    # Assume it's a dictionary with an 'annotations' key\n",
    "    for annotation in coco_data.get('annotations', []):\n",
    "        annotation['category_id'] += 1\n",
    "\n",
    "# Save the modified JSON to a new file\n",
    "with open(output_json, 'w') as file:\n",
    "    json.dump(coco_data, file, indent=4)\n",
    "\n",
    "print(f\"All category IDs have been incremented by 1 and saved to {output_json}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Output evaulation metrics in \"main_Confusion_matrix_OD.ipynb\"\n",
    "\n",
    "e.g., mAP, TP, FP, FN, precision, recall, F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BG-je0V6uCn"
   },
   "source": [
    "- Predictions are returned as [sahi.prediction.PredictionResult](sahi/prediction.py), you can access the object prediction list as:"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Detectron2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('sahi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "f2680c47b11e1b3873482f0b7ab37c9292181f84f7b4413a77eb41d52d22c05d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
